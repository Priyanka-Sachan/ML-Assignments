# -*- coding: utf-8 -*-
"""ML-Assignment-4.ipynb

Automatically generated by Colaboratory.

Original file is located at
    https://colab.research.google.com/drive/1iRAtypgRgNOQ0lCjBtZ1Ut75TgsKTpDL

## Import packages & dataset
"""

# Commented out IPython magic to ensure Python compatibility.
import torch
import torchvision
import numpy as np
import matplotlib.pyplot as plt
from torch.autograd import Variable
import torch.nn as nn
import torch.nn.functional as F
from torchvision.datasets import CIFAR10
from torchvision.transforms import ToTensor
from torchvision.utils import make_grid
from torch.utils.data.dataloader import DataLoader
from torch.utils.data import random_split
# %matplotlib inline

dataset = CIFAR10(root='data/', download=True, transform=ToTensor())
test_dataset = CIFAR10(root='data/', train=False, transform=ToTensor())

"""## Dataset pre-processing"""

dataset_size = len(dataset)
print('Dataset size:',dataset_size)
test_dataset_size = len(test_dataset)
print('Test Dataset size:',test_dataset_size)

classes = dataset.classes
num_classes = len(classes)
print('No. of classes: ',num_classes)
print('Dataset is classified as: ',classes)

img, label = dataset[0]
plt.imshow(img.permute((1, 2, 0)))
print('Where each image is',img.size())
print('Label (numeric):', label)
print('Label (textual):', classes[label])

input_size = 3*32*32
output_size = 10

# For deterministic reults
torch.manual_seed(9)

# Dividing dataset into - train and validation datset
val_size = 5000
train_size = dataset_size-val_size
train_ds, val_ds = random_split(dataset, [train_size, val_size])

print('Train dataset size:', len(train_ds))
print('Validation dataset size:', len(val_ds))

# Hyperparameters
batch_size=128
no_of_epochs=20
learning_rate=[0.01,0.033,0.067,0.1]

# Using stochastic gradient descent with batch_size=128
train_loader = DataLoader(train_ds, batch_size, shuffle=True, num_workers=2, pin_memory=True)
val_loader = DataLoader(val_ds, batch_size*2, num_workers=2, pin_memory=True)
test_loader = DataLoader(test_dataset, batch_size*2, num_workers=2, pin_memory=True)

"""## Feed Forward Neural Network (FFNN)

### Model
"""

class CIFAR10Model(nn.Module):
    def __init__(self):
        super().__init__()
        self.linear1 = nn.Linear(input_size,1024)
        self.linear2 = nn.Linear(1024,512)
        self.linear3 = nn.Linear(512,256)
        self.linear4 = nn.Linear(256,128)
        self.linear5 = nn.Linear(128,64)
        self.linear6 = nn.Linear(64,32)
        self.linear7 = nn.Linear(32,10)
       
    def forward(self, xb):
        # Flatten images into vectors
        out = xb.view(xb.size(0), -1)
        out = self.linear1(out)
        out = F.relu(out)
        out = self.linear2(out)
        out = F.relu(out)
        out = self.linear3(out)
        out = F.relu(out)
        out = self.linear4(out)
        out = F.relu(out)
        out = self.linear5(out)
        out = F.relu(out)
        out = self.linear6(out)
        out = F.relu(out)
        out = self.linear7(out)
        return out
      
    def training_step(self, batch):
        images, labels = batch 
        out = self(images)                  # Generate predictions
        loss = F.cross_entropy(out, labels) # Calculate loss
        return loss
    
    def validation_step(self, batch):
        images, labels = batch 
        out = self(images)                    # Generate predictions
        loss = F.cross_entropy(out, labels)   # Calculate loss
        acc = accuracy(out, labels)           # Calculate accuracy
        return {'val_loss': loss.detach(), 'val_acc': acc}
        
    def validation_epoch_end(self, outputs):
        batch_losses = [x['val_loss'] for x in outputs]
        epoch_loss = torch.stack(batch_losses).mean()   # Combine losses
        batch_accs = [x['val_acc'] for x in outputs]
        epoch_acc = torch.stack(batch_accs).mean()      # Combine accuracies
        return {'val_loss': epoch_loss.item(), 'val_acc': epoch_acc.item()}
    
    def epoch_end(self, epoch, result):
        print("Epoch [{}], loss: {:.4f}, accuracy: {:.4f}".format(epoch, result['val_loss'], result['val_acc']))

"""### Training"""

def accuracy(outputs, labels):
    _, preds = torch.max(outputs, dim=1)
    return torch.tensor(torch.sum(preds == labels).item() / len(preds))

def evaluate(model, val_loader):
    outputs = [model.validation_step(batch) for batch in val_loader]
    return model.validation_epoch_end(outputs)

def fit(epochs, lr, model, train_loader, val_loader, opt_func=torch.optim.SGD):
    history = []
    optimizer = opt_func(model.parameters(), lr)
    for epoch in range(epochs):
        # Training Phase 
        for batch in train_loader:
            loss = model.training_step(batch)
            loss.backward()
            optimizer.step()
            optimizer.zero_grad()
        # Validation phase
        result = evaluate(model, val_loader)
        model.epoch_end(epoch, result)
        history.append(result)
    return history

learning_rate=0.1

model = CIFAR10Model()
history = [evaluate(model, val_loader)]
print('\nTraining for learning rate: ',learning_rate)
history = fit(no_of_epochs,learning_rate, model, train_loader, val_loader)

history={}
for i in range(len(learning_rate)):
  model = CIFAR10Model()
  print('\nTraining for learning rate: ',learning_rate[i])
  history['model_'+str(i)] = fit(no_of_epochs,learning_rate[i] , model, train_loader, val_loader)

def plot_losses(history):
    losses = [x['val_loss'] for x in history]
    plt.plot(losses, '-x')
    plt.xlabel('epoch')
    plt.ylabel('loss')
    plt.title('Loss vs. No. of epochs');

def plot_accuracies(history):
    accuracies = [x['val_acc'] for x in history]
    plt.plot(accuracies, '-x')
    plt.xlabel('epoch')
    plt.ylabel('accuracy')
    plt.title('Accuracy vs. No. of epochs');

"""### Results"""

plot_losses(history['model_3'])

plot_accuracies(history['model_3'])

test = evaluate(model, test_loader)

test

torch.save(model.state_dict(), 'cifar10-ffnn.pth')

"""### Recurrent Neural Network Model (RNN)

#### Model
"""

#Hyperparameters
sequence_length=3*32
input_size=32
hidden_size=1024
no_of_epochs=50
batch_size=50
output_size=10

class ImageRNN(nn.Module):
    def __init__(self, batch_size,num_layers, sequence_length, input_size, hidden_size, output_size):
        super(ImageRNN, self).__init__()
        
        self.hidden_size = hidden_size
        self.batch_size = batch_size
        self.sequence_length = sequence_length
        self.input_size = input_size
        self.output_size = output_size
        self.num_layers=num_layers
        
        self.basic_rnn = nn.RNN(self.input_size, self.hidden_size,self.num_layers,batch_first=True) 
        
        self.FC = nn.Linear(self.hidden_size, self.output_size)
        
    def init_hidden(self,):
        # (num_layers, batch_size, hidden_size)
        return (torch.zeros(self.num_layers, self.batch_size, self.hidden_size))
        
    def forward(self, X):
        
        # self.batch_size = X.size(1)
        self.batch_size = X.size(0)
        self.hidden = self.init_hidden()
        
        # lstm_out=seq_len x batch_size x hidden_size  
        # self.hidden =num_layers x batch_size x hidden_size
        lstm_out, self.hidden = self.basic_rnn(X, self.hidden) 

        lstm_out = self.FC(lstm_out[:, -1, :]) 
        return lstm_out

import torch.optim as optim

# Device
device = torch.device("cuda:0" if torch.cuda.is_available() else "cpu")

# Model instance
model = ImageRNN(batch_size, sequence_length, input_size, hidden_size, output_size)
criterion = nn.CrossEntropyLoss()
optimizer = optim.Adam(model.parameters(), lr=0.001)

def get_accuracy(logit, target, batch_size):
    ''' Obtain accuracy for training round '''
    corrects = (torch.max(logit, 1)[1].view(target.size()).data == target.data).sum()
    accuracy = 100.0 * corrects/batch_size
    return accuracy.item()

# Commented out IPython magic to ensure Python compatibility.
for epoch in range(no_of_epochs):  # loop over the dataset multiple times
    train_running_loss = 0.0
    train_acc = 0.0
    model.train()
    
    # TRAINING ROUND
    for i, data in enumerate(train_loader):
         # zero the parameter gradients
        optimizer.zero_grad()
        
        # reset hidden states
        model.hidden = model.init_hidden() 
        
        # get the inputs
        inputs, labels = data
        inputs = inputs.view(-1, 3*32,32) 

        # forward + backward + optimize
        outputs = model(inputs)

        loss = criterion(outputs, labels)
        loss.backward()
        optimizer.step()

        train_running_loss += loss.detach().item()
        train_acc += get_accuracy(outputs, labels, batch_size)
         
    model.eval()
    print('Epoch:  %d | Loss: %.4f | Train Accuracy: %.2f' 
#           %(epoch, train_running_loss / i, train_acc/i))